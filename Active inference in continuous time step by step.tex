\documentclass[a4paper, 10pt]{article}

\usepackage[T1]{fontenc} % To choose the font encoding of the output text. You might need it if you are writing documents in a language other than English.

\usepackage[utf8]{inputenc} % To choose the encoding of the input text. You might need it if you are writing documents in a language other than English (foundamental in particular for letters with accent mark).

\usepackage[english, italian]{babel} % It provides the internationalization of LaTeX. It has to be loaded in any document, and you have to give as an option the main language you are going to use in the document.

\usepackage[a4paper, total={6in, 8in}]{geometry} % To modify margins

\newcommand{\virgolette}[1]{``#1''} % New function to use correct open and close inverted commas

\usepackage{hyperref} % It gives LaTeX the possibility to manage links within the document or to any URL when you compile in PDF.

\usepackage[autostyle]{csquotes} % Package necessary to biblatex
\usepackage[sorting=nyt, style=authoryear]{biblatex} % Advanced bibliography handling. It is the package to use for writing a thesis. bibstyle=authortitle, backend=biber
\addbibresource{activeinference.bib} % File from which take the bibliography 

\usepackage{amsmath,amsfonts,amsthm,bm} % Math packages

\usepackage{comment}

\setlength{\parskip}{0.5em}

%\setcounter{secnumdepth}{0}

\title{Active Inference in continuous time notes}
\author{Federico Maggiore}















\begin{document}
\selectlanguage{english}

\maketitle

\newpage

\tableofcontents

\newpage

\paragraph{\textbf{Summary notation}}
\begin{itemize}

\item $\mathbf x = \lbrace{ x_i \rbrace}_{i=1}^{D}$ environmental variables of the $D$-dimensional space constituting latent or hidden states;

\item $\mathbf{s} = \lbrace{ s_i \rbrace}_{i=1}^{S}$ body sensors input;

\item $\bm \mu = \lbrace \mu_i \rbrace_{i=1}^D$ inner brain state representing the hidden environmental variables $\mathbf x$

\item $P(\mathbf{x},\mathbf{s})$ \emph{Joint density};

\item $P(\mathbf{x}|\mathbf{s})$ \emph{Posterior}; 

\item $P(\mathbf{s}|\mathbf{x})$ \emph{Likelihood};

\item $P(\mathbf x)$ \emph{Prior};

\item $P(\mathbf s)=\int P(\mathbf s|\mathbf x)P(\mathbf x) d\mathbf x $ \emph{marginal likelihood};

\item $Q(\mathbf x)$ \emph{R-density};

\item $F \equiv \int Q(\mathbf x) \ln \frac{Q(\mathbf x)}{P(\mathbf x,\mathbf s)}d\mathbf x $ \emph{Variational Free Energy};

\item $L(\mu,s) \equiv - \ln P(\mu,s)$ \emph{Laplace-encoded energy};

\item $\varepsilon$ \emph{Prediction error};

\item $\Sigma^{-1}$ \emph{Precision}
\end{itemize}

\paragraph{\textbf{References:}} 
\cite{Baltieri2019}, \cite{Buckley2017}, \cite{Bogacz2017},




\section{Free Energy Principle (FEP)}
The goal of an agent is to determine the probability of the hidden states given some sensory inputs:

\begin{equation}
P(\mathbf{x}|\mathbf{s}) = \frac{P(\mathbf{x},\mathbf{s})}{P(\mathbf{s})} = \frac{P(\mathbf{s}|\mathbf{x})P(\mathbf{x})}{P(\mathbf{s})}
\end{equation}
with
\begin{itemize}

\item $P(\mathbf{x},\mathbf{s})$ \emph{joint density}, beliefs about the states assumed to be encoded by the agent;

\item $P(\mathbf{x}|\mathbf{s})$ \emph{Posterior}, i.e. probability of hidden causes $x$ given observed sensory data; 

\item $P(\mathbf{s}|\mathbf{x})$ \emph{Likelihood}, i.e. organism's assumptions about sensory input $\mathbf{s}$ given the hidden causes $\mathbf{x}$;

\item $P(\mathbf x)$ \emph{Prior}, i.e. agent's beliefs about hidden causes \textbf{before} that $\mathbf s$ are received;

\item $P(\mathbf s)=\int P(\mathbf s|\mathbf x)P(\mathbf x) d\mathbf x $ \emph{marginal likelihood}, i.e. normalization factor.

\end{itemize}

For the agent it's not necessary to compute the complete posterior distribution, it has only to find the hidden state -or at least a good approximation- that maximize the posterior, i.e. $\arg \max_{\mathbf{x}} P(\mathbf x|\mathbf s)$.
The problem with the exact Bayesian scheme, is that $P(\mathbf s)$ is often impossible to calculate, and moreover $P(\mathbf x|\mathbf s)$ may not take a standard shape and could not have a summary statistics. 

A biologically plausible technique consist in using an auxiliary distribution $Q(\mathbf x)$ called \emph{recognition density} (\emph{R-density}) that has to be optimized to became a good approximation of the posterior.  

In order to do this the Kullback-Leibler  divergence is minimized:
\begin{equation}
\begin{split}
D_{KL} (\, Q(\mathbf x)\, ||\, P(\mathbf x|\mathbf s)\, ) & = \int Q(\mathbf x) \ln \frac{Q(\mathbf x)}{P(\mathbf x|\mathbf s)} d\mathbf x \\
                                  & = \int Q(\mathbf x) \ln \frac{Q(\mathbf x)P(\mathbf s)}{P(\mathbf x,\mathbf s)} d\mathbf x \\
                                  & = \int Q(\mathbf x) \ln \frac{Q(\mathbf x)}{P(\mathbf x,\mathbf s)} d\mathbf x + \ln P(\mathbf s) \int Q(\mathbf x) d\mathbf x \\
                                  & = F + \ln P(\mathbf s)
\end{split}
\end{equation}

where
\begin{itemize}

\item $F \equiv \int Q(\mathbf x) \ln \frac{Q(\mathbf x)}{P(\mathbf x,\mathbf s)}d\mathbf x = - \left< \ln P(\mathbf x,\mathbf s) \right>_{Q} + \left< \ln Q(\mathbf x) \right>_{Q}$ is the \emph{Variational Free Energy} (VFE), a quantity that depends on the R-density and the knowledge about the environment i.e. the joint density $P(\mathbf s, \mathbf x) = P(\mathbf s|\mathbf x)P(\mathbf x)$ that we are assuming the agent has. 
\item $\ln P(\mathbf s)$ is a term independent with respect to the recognition density $Q(\mathbf x)$ ( $\Rightarrow$ minimizing F with respect to $Q(\mathbf x)$ will minimize the $D_{KL}$ )

\end{itemize}

\section{One dimensional case}
\label{sec:1d}
For the sake of simplicity, let's build the framework first in the one dimensional case, to repeat later on all the steps for the multivariate case.

\subsection{Laplace approximation}
\label{sec:laplace}

Often optimizing $F$ for arbitrary $Q(x)$ is particularly complex. Moreover, it is assumed that neural activity parametrise sufficient statistic.
For these reasons, a common approximation is to assume that the R-density take a Gaussian form. 

Let us assume that the R-density $Q(x)$ has a peak at point $\mu$. The Taylor-expansion of the logarithm around this peak is
\begin{equation}
\ln Q(x) \simeq \ln Q(\mu) - \frac{1}{2} \frac{(x-\mu)^2}{\Sigma} 
\end{equation}
with
\begin{equation}
\frac{1}{\Sigma} = - \frac{\partial^{2} }{\partial x^2} \ln Q(x) \bigg\rvert_{x=\mu} 
\end{equation}
Now it is possible to approximate the probability distribution $Q(x)$ with the distribution
\begin{equation}
\mathcal{N}(x;\mu, \Sigma) = \frac{1}{\sqrt{ 2 \pi \Sigma}} \, e^{\frac{(x-\mu)^2}{2 \Sigma}}
\end{equation}
i.e. a Gaussian distribution that has been normalized using the factor $Q(\mu) \sqrt{2 \pi \Sigma }$.\\
Now the VFE can be written as follow
\begin{equation}
\begin{split}
F   & \approx \int \mathcal{N}(x;\mu,\Sigma) (-\frac{1}{2} \ln (2 \pi \Sigma) - \frac{(x-\mu)^2}{2 \Sigma} ) d x - \int \mathcal{N}(x;\mu,\Sigma) \ln P(x,s) d x \\
    & = -\frac{1}{2} \ln (2 \pi \Sigma) - \frac{1}{2 \Sigma} \int \mathcal{N}(x;\mu,\Sigma) (x-\mu)^2  d x - \int \mathcal{N}(x;\mu,\Sigma) \ln P(x,s) d x \\
    & = -\frac{1}{2} \ln (2 \pi \Sigma) - \frac{1}{2} - \int \mathcal{N}(x;\mu,\Sigma) \ln P(x,s) d x
\end{split}
\end{equation}
To end up with an analityc model of the FEP, further simplifications and assumptions are needed to evaluate the last term\footnote{At the end we expand the implications for the interpretation of brain functions due to this issue.}. \\
Let us first assume that the R-density is sharply peaked at its mean value and that $P(x,s)$ is a smooth function of $x$: under these assumptions is possible to consider the integrated function appreciably non-zero only near the peak, and is possible to use a second order Taylor expansion of the $L(x,s) \equiv - \ln P(x,s)$ around $x=\mu$.
\begin{equation}
L(x,s) \approx L(\mu,s) + \left[ \frac{dL(x,s)}{dx} \right]_{x=\mu} (x-\mu) + \frac{1}{2} \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu} (x-\mu)^2 \\
\end{equation}
\begin{center}
$\Downarrow$
\end{center}
\begin{equation}
\begin{split}
- \int \mathcal{N}(x;\mu,\Sigma) \ln P(x,s) d x \approx & \int \mathcal{N}(x;\mu,\Sigma) \Big\lbrace L(\mu,s) + \left[ \frac{dL(x,s)}{dx} \right]_{x=\mu} (x-\mu) \, + \\
													  & + \frac{1}{2} \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu} (x-\mu)^2 \Big\rbrace d x \\
											  		= & \, L(\mu,s) + \left[ \frac{dL(x,s)}{dx} \right]_{x=\mu} \left( \int \mathcal{N}(x;\mu,\Sigma) x \, dx - \mu \right) + \\
											  		& + \frac{1}{2} \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu} \int \mathcal{N}(x;\mu,\Sigma) (x-\mu)^2 \, dx\\
											  		= & \, L(\mu,s) + \frac{1}{2} \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu} \Sigma
\end{split}
\end{equation}
Now is possible to rewrite the variational free energy as
\begin{equation}
F(\mu, \Sigma, s) \approx L(\mu, s) + \frac{1}{2}\left( \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu} \Sigma - \ln (2 \pi \Sigma) -1 \right)
\end{equation}
with $L(\mu, s)$ said \emph{Laplace-encoded energy}, and the variational free energy written as a function and not anymore as a functional.

Since the goal is to minimize the Kullback-Leibler divergence trough the minimization of the VFE, is possible to simplify further removing the $\Sigma$ dependency taking the derivative with respect this and imposing $\frac{dF}{d\Sigma}=0$
\begin{equation}
\frac{dF}{d\Sigma}= \frac{1}{2} \left( \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu} - \frac{1}{\Sigma} \right) = 0
\end{equation}
\begin{equation}
\Rightarrow \Sigma = \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu}^{-1} \equiv \Sigma^{\ast}
\end{equation}

The final form of the VFE is then
\begin{equation}
F \approx L(\mu,s) - \frac{1}{2} \ln \left( 2 \pi \Sigma^{\ast} \right) \, ,
\end{equation}
that can be further simplified if the $L$ function has a second order polynomial form\footnote{as we will see in Sec.(\ref{sec:G})}, that implies that the second-order derivative of $L$ with respect to $x$ results in a constant factor that is useless and can be ignored\footnote{ let us remind that the final goal is to minimize $F$ with respect to $x$}, leading to 
\begin{equation}
F \approx L(\mu,s)
\label{eqn:encoded_F}
\end{equation}


\paragraph{About Laplace Approximation}
Carrying on a theory based on an approximation that leads to Eq.(\ref{eqn:encoded_F}), means to assume that the brain represents only the most likely environmental cause of sensory data and nothing else about the distribution. However, as we're going to see, the uncertainties are encoded directly in the form of the joint density.



\subsection{Building the generative model}
\label{sec:G}
Thanks to the Laplace approximation, we've been able to write the VFE in terms of the Laplace-encoded energy $L(\mu,s)$, that in turn depends on the joint density $P(x, s)$. In this function are encoded the brain beliefs about the environmental causes of the sensory input and the beliefs \emph{a priori} about environmental states.

Therefore, what needs to be built is a \emph{generative model}, that is a model in which is encoded how the brain believe the world works and where all the hypothesis about the agent's behaviour are formalized.

\subsubsection{Static Model}

Let us consider a simple case of an agent that believes in an environment with hidden state $ x$ that stimulates a sensory channel $ s$. As we've seen in Sec.(\ref{sec:laplace}), the brain will represent the environment only through the inner state $\mu$, and what remains to do is to explicit the mapping between brain states and sensory data that will allows to make explicit the joint density.

Let us assume that the agent believes its sensory input are generated by
\begin{equation}
s = g(x) + \mathcal{N}(s;0,\Sigma_s) \, ,
\label{eqn:input}
\end{equation}
with $g$ generic function that expresses the relation between states and sensory input, to which is summed a noise represented by the normal distribution with zero mean and variance $\Sigma_s$. This assumption means that we can write
\begin{equation}
P(s|x) = \mathcal{N}(s;g(x),\Sigma_s) = \frac{1}{\sqrt{ 2 \pi \Sigma_{s}}} \, e^{\frac{(s-g(x))^2}{2 \Sigma_{s}}} \, .
\end{equation}
Moreover let us also assume that the agent also has a prior knowledge regarding the environmental state given by $\bar{\mu}$ that is linked with the inner state through
\begin{equation}
x = \bar{\mu} + \mathcal{N}(x;0,\Sigma_{\bar{\mu}})
\end{equation}
\begin{equation}
\Rightarrow P(x) = \mathcal{N}(x;\bar{\mu},\Sigma_{\bar{\mu}}) =\frac{1}{\sqrt{ 2 \pi \Sigma_{\bar{\mu}}}} \, e^{\frac{(x-\bar{\mu})^2}{2 \Sigma_{\bar{\mu}}}} \, .
\end{equation}

Now that we have specified a likelihood and a prior, is possible to determine the joint density
\begin{equation}
P(x,s) = P(s | x) P(x)
\end{equation}
and consequently the Laplace-encoded energy
\begin{equation}
\begin{split}
L(\mu,s) & = - \ln P(s|\mu) - \ln P(\mu)  \\
		 & = \frac{1}{2} \ln (2 \pi \Sigma_{s}) + \frac{(s-\mu)^2}{2 \Sigma_{s}} + \frac{1}{2} \ln (2 \pi \Sigma_{\bar{\mu}}) + \frac{(\mu-\bar{\mu})^2}{2 \Sigma_{\bar{\mu}}} \\
		 & = \frac{\varepsilon_{s}^2}{2 \Sigma_{s}} + \frac{\varepsilon_{\mu}^2}{2 \Sigma_{\bar{\mu}}} + \frac{1}{2} \ln \left( \Sigma_{s} \Sigma_{\bar{\mu}} \right) + \ln (2 \pi) \, ,
\end{split}
\end{equation}
where the $\varepsilon$ terms are said \emph{prediction errors} and measure the discrepancy respectively between the actual sensory data $s$ and the outcome of its prediction $g(\mu)$ and between $\mu$ itself and its prior expectation $\bar{\mu}$. Therefore the former $\varepsilon_{s}$ describes sensory prediction errors, the latter $\varepsilon_{\mu}$ model prediction errors (i.e. how brain states deviate from their expectation) and each one is weighted with the the corresponding inverse of the variance $\Sigma_{s}^{-1}$ and $\Sigma_{\mu}^{-1}$ (which are often said \emph{precisions}). \\
As said at the end of Sec.(\ref{sec:laplace}), since the $L$ function has a quadratic form is possible to ignore all the terms apart from the following
\begin{equation}
F \approx \frac{\varepsilon_{s}^2}{2 \Sigma_{s}} + \frac{\varepsilon_{\mu}^2}{2 \Sigma_{\bar{\mu}}} + \frac{1}{2} \ln \left( \Sigma_{s} \Sigma_{\bar{\mu}} \right) \, .
\end{equation}


\subsubsection{Dynamic Model}
Let's formulate a possible implementation of inference in a dynamically changing environment.

As the static case, is assumed that the agent believes its sensory input are generated in a similar manner with respect to Eq.(\ref{eqn:input}), in particular
\begin{equation}
s(x,t) = g(x) + z(t) \, .
\label{eqn:s}
\end{equation}
Additionally, is assumed that the agent model of environmental dynamic follows the Langevin-type equation
\begin{equation}
\frac{d x(t)}{dt} = f(x) + w(t) \, .
\label{eqn:x}
\end{equation}
Both $z$ and $w$ are terms representing noise and that we will specify later.

Using a \emph{generalised state-space model}, in which the state of a dynamical system is represented in terms of increasingly higher order derivative of its state variables, in combination with local linearity approximation on higher orders of motion\footnote{Without this approximation the model would scale-up very quickly becoming complicated and unwieldy fairly quickly. This approximation becomes exact when $f$ and $g$ are linear.} suppressing non-linear terms in the partial derivatives, is possible to obtain 
\begin{equation}
  \begin{split}
    s &= g(x) + z(t) \\
    s' &= \frac{\partial g}{\partial x}x' + z' \\
    s'' & \simeq \frac{\partial g}{\partial x}x'' + z''\\
    				& \qquad \vdots
  \end{split}
  \qquad
  \begin{split}
    x' &= f(x) + w(t) \\
    x'' &= \frac{\partial f}{\partial x}x' + w' \\
    x''' & \simeq \frac{\partial f}{\partial x}x'' + w''\\
    				& \qquad \vdots
  \end{split}
\label{eqn:hdm}
\end{equation}
where we have used the notation
\begin{equation}
s' = \frac{ds}{dt} \, , \qquad x'=\frac{dx}{dt} \, , \qquad s''= \frac{d^2 s}{dt^2} \, , \qquad x''= \frac{d^2 x}{dt^2} \, , \quad \dots
\end{equation}
and where $z,z',z'',\dots, w,w',w'',\dots$ are the noises source at each dynamic order.
Considering the previous linear approximation as equalities, Eq.(\ref{eqn:hdm}) can be expressed in the more compact form
\begin{equation}
\tilde{s} = g(\tilde{x}) + \tilde{z} \qquad \tilde{x}' = f(\tilde{x}) + \tilde{w}
\end{equation}
using the notation $\tilde{s} \equiv \dots$
\paragraph{Considerations about noise sources}
the stochastic terms $z(t)$ and $w(t)$ in Eq.(\ref{eqn:s}) and Eq.(\ref{eqn:x}) are analytic and form stochastic equations based on Stratonovich calculus, with strictly non-zero and well defined covariances of . This property is important to define a non-Markovian proces, because in Ito's formulation, based on Wiener noise, the covariances are delta functions and represent perfect white noise not existing in real world. This formulation can be seen as an extension of the standard state-space models using in particular a colored noise.



\subsection{VFE minimisation}
In Sec.(\ref{sec:laplace}) we have expressed, in an approximated form, the Variational Free Energy in terms of the Laplace-encoded Energy, which depends on joint density $P(\mathbf x, \mathbf s)$, that we have seen how to build starting from the generative model in Sec.(\ref{sec:G}). Now we are going to see a possible implementation of a biologically plausible mechanism to minimise VFE.

In the Free Energy Principle framework, it is proposed that the innate dynamics of the neural activity evolves in such a way that it implement a gradient descent scheme on the VFE. 










\newpage
\section{Multivariate case}
Now that we have seen how to organize an active inference framework in the one dimensional case, let's see how to scale up the dimensions of our system remembering that $\mathbf x = \lbrace x_i \rbrace_{i=1}^D$, $\mathbf s = \lbrace s_i \rbrace_{i=1}^S$ and $\bm \mu = \lbrace \mu_i \rbrace_{i=1}^D$.

Let's recall that the goal is to minimize the Kullback-Leibler  divergence
\begin{equation}
D_{KL} (\, Q(\mathbf x)\, ||\, P(\mathbf x|\mathbf s)\, )  =  F + \ln P(\mathbf s) = - \left< \ln P(\mathbf x,\mathbf s) \right>_{Q} + \left< \ln Q(\mathbf x) \right>_{Q} 
\end{equation}
that, since $P(\mathbf s)$ is independent with respect to the recognition density $Q(\mathbf x)$, is equivalent to minimize the Variational Free Energy
\begin{equation}
F = \int Q(\mathbf x) \ln Q(\mathbf x) d\mathbf x - \int Q(\mathbf x) \ln P(\mathbf x,\mathbf s) d\mathbf x
\end{equation}


\subsection{Laplace approximation}
As done before, let's start assuming that neural activity parametrise sufficient statistic, in particular let's approximate the R-density $Q(\mathbf x)$ with a multivariate Gaussian over the D-dimensional space $\mathbf x$, with peak at $\boldsymbol \mu$, and let's do the same procedure done in Sec.(\ref{sec:laplace}).

\begin{equation}
\ln Q(\mathbf x) \simeq \ln Q(\boldsymbol{\mu}) - \frac{1}{2} (\mathbf x-\boldsymbol \mu)^T \boldsymbol{\hat{\Sigma}}^{-1} (\mathbf x-\boldsymbol \mu)
\end{equation}
with
\begin{equation}
\left[ \boldsymbol{\hat{\Sigma}}^{-1} \right]_{i,j} = - \frac{\partial^{2} }{\partial x_i \partial x_j} \ln Q(\mathbf x) \bigg\rvert_{\mathbf x=\boldsymbol \mu} 
\end{equation}
Now let us approximate $Q(\mathbf x)$ with the multivariate Gaussian distribution
\begin{equation}
\mathcal{N}(\mathbf x;\boldsymbol \mu, \boldsymbol{\hat{\Sigma}}) = \frac{1}{\sqrt{( 2 \pi)^{D} \det \boldsymbol{\hat{\Sigma}}}} \, e^{ - \frac{1}{2} (\mathbf x-\boldsymbol \mu)^T \boldsymbol{\hat{\Sigma}}^{-1} (\mathbf x-\boldsymbol \mu)}
\end{equation}
and rewrite the VFE as follow:

\begin{equation}
\label{eqn:f1}
\begin{split}
F \approx & \int \mathcal{N}(\mathbf x; \bm \mu, \bm{\hat{\Sigma}}) \left[ -\frac{1}{2} \ln \left( (2 \pi)^{D} \det \bm{\hat{\Sigma}} \right) - \frac{1}{2} (\mathbf x-\boldsymbol \mu)^T \boldsymbol{\hat{\Sigma}}^{-1} (\mathbf x-\boldsymbol \mu) \right] d \mathbf x \, + \\
	& - \int \mathcal{N}(\mathbf x;\bm \mu,\bm{\hat{\Sigma}}) \ln P(\mathbf x,\mathbf s) d \mathbf x \\
    = & -\frac{1}{2} \ln \left( (2 \pi)^{D} \det \bm{\hat{\Sigma}} \right) - \int \mathcal{N}(\mathbf x; \bm \mu, \bm{\hat{\Sigma}}) \left[ \frac{1}{2} (\mathbf x-\boldsymbol \mu)^T \boldsymbol{\hat{\Sigma}}^{-1} (\mathbf x-\boldsymbol \mu) \right] d \mathbf x \, + \\
    & - \int \mathcal{N}(\mathbf x;\bm \mu,\bm{\hat{\Sigma}}) \ln P(\mathbf x,\mathbf s) d \mathbf x 
\end{split}
\end{equation}

Let us focus on the second term making as first thing the change of variables with unitary Jacobian $\mathbf y = \mathbf x - \bm \mu$. 
\begin{equation}
\int \mathcal{N}(\mathbf x; \bm \mu, \bm{\hat{\Sigma}}) \left[ \frac{1}{2} (\mathbf x-\boldsymbol \mu)^T \boldsymbol{\hat{\Sigma}}^{-1} (\mathbf x-\boldsymbol \mu) \right] d \mathbf x = 
\int \mathcal{N}(\mathbf y; \bm 0, \bm{\hat{\Sigma}}) \left[ \frac{1}{2} \mathbf y^T \boldsymbol{\hat{\Sigma}}^{-1} \mathbf y \right] d \mathbf y
\end{equation}
After that, since $\bm{\hat{\Sigma}} ^{-1}$ is a symmetric and real matrix, the spectral theorem guarantees the existence of an orthonormal matrix $\mathbf U$ such that $ \mathbf U^{T} \mathbf{\hat{\Sigma}}^{-1} \mathbf U = \bm \Lambda$, with $\bm \Lambda$ diagonal matrix containing the eigenvalues $\lbrace \lambda_i \rbrace_{i=1}^D$ of $\bm{\hat{\Sigma}}^{-1}$ and $\mathbf U$ containing as columns the eigenvectors of $\bm{\hat{\Sigma}}^{-1}$, so adding the identity matrix $\mathbf I = \mathbf U \mathbf U^{T}$ we get
\begin{equation}
\mathbf y^{T} \bm{\hat{\Sigma}}^{-1} \mathbf y = \mathbf y^{T} \mathbf U \mathbf U^{T} \bm{\hat{\Sigma}}^{-1} \mathbf U \mathbf U^{T} \mathbf y = \mathbf z^{T} \bm \Lambda \mathbf z = \sum_{i=1}^{D} \lambda_i z_i^2  \, ,
\end{equation}
where $\mathbf z = \mathbf U^{T} \mathbf y$ is the $\mathbf y$ representation in the orthonormal basis given by the eigenvectors of $\bm \Sigma^{-1}$. Therefore moving to the variable $\mathbf z$ (the Jacobian of this change of variables is unitary too), we obtain
\begin{equation}
\frac{1}{\sqrt{( 2 \pi)^{D} \det \boldsymbol{\hat{\Sigma}}}} \, \int \frac{1}{2} \left[ \sum_{i=1}^{D} \lambda_i z_i^2 \right] \, e^{ - \frac{1}{2} \sum_{i=1}^D \lambda_i z_i^2 } d \mathbf z = \frac{1}{2} \sqrt{\frac{\prod_{i=1}^D \lambda_i}{( 2 \pi)^{D}}} \, \sum_{i=1}^{D} \int \lambda_i z_i^2 \, e^{ - \frac{1}{2} \sum_{i=1}^{D} \lambda_i z_i^2} d z_{i} 
\end{equation}
indicating with $\mathbf z_{\not{ \, i}} \equiv \lbrace z_j \rbrace_{j \neq i}$
\begin{multline}
\frac{1}{2} \sqrt{\frac{\prod_{i=1}^D \lambda_i}{( 2 \pi)^{D}}} \, \sum_{i=1}^{D} \int \lambda_i z_i^2 \, e^{ - \frac{1}{2} \lambda_i z_i^2} d z_i \int e^{\frac{1}{2} \sum_{j \neq i} \lambda_i z_j^2} d \mathbf z_{\not{\, i}} = \\
= \frac{1}{2} \sqrt{\frac{\prod_{i=1}^D \lambda_i}{( 2 \pi)^{D}}} \, \sum_{i=1}^{D} \lambda_i \sqrt{\frac{2 \pi}{\lambda_i}} \frac{1}{\lambda_i} \, \sqrt{\frac{(2\pi)^{D-1}}{\prod_{\not{\, i}} \lambda_{\not{\, i}}}} = \frac{1}{2} \sqrt{\frac{\prod_{i=1}^D \lambda_i}{( 2 \pi)^{D}}} \, \sum_{i=1}^{D} \sqrt{\frac{(2 \pi)^D}{\prod_{i=1}^D \lambda_i}}  = \frac{D}{2}
\end{multline}
In conclusion then
\begin{gather}
\int \mathcal{N}(\mathbf x; \bm \mu, \bm{\hat{\Sigma}}) \left[ \frac{1}{2} (\mathbf x-\boldsymbol \mu)^T \boldsymbol{\hat{\Sigma}}^{-1} (\mathbf x-\boldsymbol \mu) \right] d \mathbf x = \frac{D}{2} \\
\Rightarrow F \approx -\frac{1}{2} \ln \left( (2 \pi)^{D} \det \bm{\hat{\Sigma}} \right) - \frac{D}{2} - \int \mathcal{N}(\mathbf x;\bm \mu,\bm{\hat{\Sigma}}) \ln P(\mathbf x,\mathbf s) d \mathbf x 
\end{gather}

Let us now evaluate the last term. As done before, let us assume that the R-density is sharply peaked at its mean value (i.e. both variance of and covariances between variables are small) and that $P(\mathbf x, \mathbf s)$ is a smooth function of $\mathbf x$: under these assumptions is possible to consider the integrated function appreciably non-zero only near the peak, and is possible to use a second order Taylor expansion of the $L(\mathbf x,\mathbf s) \equiv - \ln P(\mathbf x,\mathbf s)$ around $\mathbf x=\bm \mu$.
\begin{equation}
L(\mathbf x, \mathbf s) \approx L(\bm \mu, \mathbf s) +  (\mathbf x-\bm \mu)^T \left[ \nabla L(\mathbf x,\mathbf s) \right]_{\mathbf x=\bm \mu} + \frac{1}{2} (\mathbf x-\bm \mu)^T \left[ \nabla^2 L(\mathbf x, \mathbf s) \right]_{x=\mu} (\mathbf x-\bm \mu)
\end{equation}
\begin{center}
$\Downarrow$
\end{center}
\begin{equation}
\begin{split}
- \int \mathcal{N}(\mathbf x;\bm \mu,\bm{\hat{\Sigma}}) \ln P(\mathbf x,\mathbf s) d \mathbf x \approx & \int \mathcal{N}(\mathbf x;\bm \mu,\bm{\hat{\Sigma}}) \Big\lbrace L(\bm \mu, \mathbf s) +  (\mathbf x-\bm \mu)^T \left[ \nabla L(\mathbf x,\mathbf s) \right]_{\mathbf x=\bm \mu} + \\
	& + \frac{1}{2} (\mathbf x-\bm \mu)^T \left[ \nabla^2 L(\mathbf x, \mathbf s) \right]_{x=\mu} (\mathbf x-\bm \mu) \Big\rbrace d \mathbf x \\
	= & \, L(\bm \mu, \mathbf s) + \left( \int \mathcal{N}(\mathbf x; \bm \mu,\bm{\hat{\Sigma}}) \mathbf x^T \, d \mathbf x - \bm \mu^T \right) \left[ \nabla L(\mathbf x,\mathbf s) \right]_{\mathbf x=\bm \mu} \\
	& + \frac{1}{2} \int \mathcal{N}(\mathbf x;\bm \mu,\bm{\hat{\Sigma}}) (\mathbf x-\bm \mu)^T \left[ \nabla^2 L(\mathbf x, \mathbf s) \right]_{x=\mu} (\mathbf x-\bm \mu) \, d \mathbf x \\
\stackrel{?}{=} &\, L(\bm \mu,\mathbf s) + \frac{D}{2} \left[ \nabla^2 L(\mathbf x, \mathbf s) \right]_{x=\mu} \bm{\hat{\Sigma}}
\end{split}
\end{equation}
\begin{comment}
Now is possible to rewrite the variational free energy as
\begin{equation}
F(\bm \mu, \bm{\hat{\Sigma}}, \mathbf s) \approx L(\bm \mu,\mathbf s) + \frac{1}{2}\left( D \left[ \nabla^2 L(\mathbf x, \mathbf s) \right]_{x=\mu} \bm - \ln \left( (2 \pi)^{D} \det \bm \Sigma \right) - D \right)
\end{equation}
with again $L(\bm \mu,\mathbf s)$ as \emph{Laplace-encoded energy}, and the variational free energy written as a function and not anymore as a functional.

Since the goal is to minimize the Kullback-Leibler divergence trough the minimization of the VFE, is possible to simplify further removing the $\Sigma$ dependency taking the derivative with respect this and imposing $\frac{dF}{d\Sigma}=0$
\begin{equation}
\frac{dF}{d\Sigma}= \frac{1}{2} \left( \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu} - \frac{1}{\Sigma} \right) = 0
\end{equation}
\begin{equation}
\Rightarrow \Sigma = \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu}^{-1} \equiv \Sigma^{\ast}
\end{equation}

The final form of the VFE is then
\begin{equation}
F \approx L(\mu,s) - \frac{1}{2} \ln \left( 2 \pi \Sigma^{\ast} \right)
\end{equation}
\end{comment}

\begin{equation}
\vdots \notag
\end{equation}

\begin{equation}
F \approx L(\bm \mu,\mathbf s)
\end{equation}






\subsection{Building the generative model}
Now that we've expressed the VFE in terms of the Laplace-encoded Energy $L(\bm \mu , \mathbf s)$, let's develop the generative model through the building of the joint density $P(\mathbf x,\mathbf s)$.

\subsubsection{Static Model}
Here the agent believes in an environment with hidden state $\mathbf x$ that stimulates a sensory channel $\mathbf s$ and that it builds the joint density using a likelihood and a prior.

The prior is built from the expectations 
\begin{equation}
\mathbf x = \bm \bar{\bm \mu} + \mathbf z_{\bar{\bm \mu}} \, ,
\end{equation}
where $\mathbf z_{\bar{\bm \mu}}$ is a $D$-dimensional vector describing correlated noise with zero mean and covariance matrix $\bm{\hat{\Sigma}}_{\bar{\bm \mu}}$ that allows us to write
\begin{equation}
P(\mathbf x) = \mathcal{N}(\mathbf x; \bar{\bm \mu}, \bm{\hat{\Sigma}}_{\bar{\bm \mu}}) = \frac{1}{\sqrt{( 2 \pi)^{D} \det \boldsymbol{\hat{\Sigma}}_{\bar{\bm \mu}}}} \, e^{ - \frac{1}{2} (\mathbf x - \bm \bar{\bm \mu})^T \bm{\hat{\Sigma}}_{\bar{\bm \mu}}^{-1} (\mathbf x - \bm \bar{\bm \mu})} \, .
\end{equation}
Similarly the sensory inputs are assumed to be generated by
\begin{equation}
\mathbf s = \bm g(\mathbf x) + \mathbf z_{\mathbf s} \, ,
\end{equation}
with again $\mathbf z_{\mathbf s}$ $S$-dimensional vector describing correlated noise with zero mean and covariance matrix $\bm{\hat{\Sigma}}_{\mathbf s}$ that brings to the likelihood
\begin{equation}
P(\mathbf s | \mathbf x) = \mathcal{N}(\mathbf s; \bm g(\mathbf x), \bm{\hat{\Sigma}}_{\mathbf s}) = \frac{1}{\sqrt{( 2 \pi)^{S} \det \boldsymbol{\hat{\Sigma}}_{\mathbf s}}} \, e^{ - \frac{1}{2} \left(\mathbf s - \bm g(\mathbf x) \right)^T \bm{\hat{\Sigma}}_{\mathbf s}^{-1} \left(\mathbf s - \bm g(\mathbf x) \right)} \, .
\end{equation}

Now is possible to write the Laplace-encoded energy
\begin{equation}
\begin{split}
L(\bm \mu, \mathbf s) = & \, \frac{1}{2} \ln \left( (2 \pi)^S \det \bm{\hat{\Sigma}}_{s} \right) + \frac{1}{2} \left(\mathbf s - \bm g(\bm \mu) \right)^T \bm{\hat{\Sigma}}_{\mathbf s}^{-1} \left(\mathbf s - \bm g(\bm \mu) \right) \\
			& + \frac{1}{2} \ln \left( (2 \pi)^D \det \bm{\hat{\Sigma}}_{\bar{\bm \mu}} \right) + \frac{1}{2} (\bm \mu - \bm \bar{\bm \mu})^T \bm{\hat{\Sigma}}_{\bar{\bm \mu}}^{-1} (\bm \mu - \bm \bar{\bm \mu}) \\
		=	& \, \frac{1}{2} \bm{\varepsilon_{\mathbf s}}^T \bm{\hat{\Sigma}}_{\mathbf s}^{-1} \bm{\varepsilon_{\mathbf s}} + \frac{1}{2} \bm{\varepsilon_{\bm \mu}}^T \bm{\hat{\Sigma}}_{\bar{\bm \mu}}^{-1} \bm{\varepsilon_{\bm \mu}} \\
			& + \frac{1}{2} \ln \left( \det \bm{\hat{\Sigma}}_{\bar{\bm \mu}} \det \bm{\hat{\Sigma}}_{s} \right) + \frac{1}{2} \ln \left( (2 \pi)^D (2 \pi)^S  \right) \, ,
\end{split}
\end{equation}
where again we have introduced the prediction errors $\bm{\varepsilon_{\mathbf s}} \equiv \mathbf s - \bm g(\bm \mu)$ and $\bm{\varepsilon_{\bm \mu}} \equiv \bm \mu - \bm \bar{\bm \mu}$ and we're able to write the VFE approximated form getting rid of all constants and the term depending on $\bm{\hat{\Sigma}}^{\star}$ :
\begin{equation}
F \approx \frac{1}{2} \bm{\varepsilon_{\mathbf s}}^T \bm{\hat{\Sigma}}_{\mathbf s}^{-1} \bm{\varepsilon_{\mathbf s}} + \frac{1}{2} \bm{\varepsilon_{\bm \mu}}^T \bm{\hat{\Sigma}}_{\bar{\bm \mu}}^{-1} \bm{\varepsilon_{\bm \mu}} + \frac{1}{2} \ln \left( \det \bm{\hat{\Sigma}}_{\bar{\bm \mu}} \det \bm{\hat{\Sigma}}_{s} \right) \, .
\end{equation}

Often, the VFE is further simplified making another assumption, that is the \emph{mean field approximation}, which implies statistical independence between environmental variables and between sensory inputs\footnote{$\Rightarrow \forall \, \, i \neq j \, \, \Sigma_{\bar{\bm \mu}}^{(i,j)}=0 \, , \Sigma_{\mathbf s}^{(i,j)}=0 $}, leading to a likelihood and a prior factorised respectively 
\begin{equation}
P(\mathbf x) = \prod_{j=1}^{D} \mathcal{N}(x_{j}; \bar{\mu}_{j}, \Sigma_{\bar{\bm \mu}}^{(j,j)})
\end{equation}
\begin{equation}
P(\mathbf s | \mathbf x) = \prod_{i=1}^{S} \mathcal{N}(s_{i}; g_{i}(\mathbf x), \Sigma_{\mathbf s}^{(i,i)})
\end{equation}
and a VFE with form
\begin{equation}
\begin{split}
F &\approx \sum_{i=1}^S \left[ \frac{(\varepsilon_{\mathbf s}^{(i)})^2}{2 \Sigma_{\mathbf s}^{(i,i)}} + \frac{1}{2} \ln \Sigma_{\mathbf s}^{(i,i)}  \right] + \sum_{j=1}^D \left[ \frac{(\varepsilon_{\bm \mu}^{(j)})^2}{2 \Sigma_{\bar{\bm \mu}}^{(j,j)}} + \frac{1}{2} \ln \Sigma_{\bar{\bm \mu}}^{(j,j)}  \right] \\
	&= \sum_{i=1}^S \left[ \frac{(\varepsilon_{\mathbf s}^{(i)})^2}{2 \Sigma_{\mathbf s}^{(i,i)}} \right] + \sum_{j=1}^D \left[ \frac{(\varepsilon_{\bm \mu}^{(j)})^2}{2 \Sigma_{\bm \mu}^{(j,j)}} \right] + \frac{1}{2} \ln \left( \det \bm{\hat{\Sigma}}_{\bar{\bm \mu}} \det \bm{\hat{\Sigma}}_{s} \right) \, .
\end{split}
\end{equation}





\newpage

\printbibliography



\end{document}


