\documentclass[a4paper, 10pt]{article}

\usepackage[T1]{fontenc} % To choose the font encoding of the output text. You might need it if you are writing documents in a language other than English.

\usepackage[utf8]{inputenc} % To choose the encoding of the input text. You might need it if you are writing documents in a language other than English (foundamental in particular for letters with accent mark).

\usepackage[english, italian]{babel} % It provides the internationalization of LaTeX. It has to be loaded in any document, and you have to give as an option the main language you are going to use in the document.

\usepackage[a4paper, total={6in, 8in}]{geometry} % To modify margins

\newcommand{\virgolette}[1]{``#1''} % New function to use correct open and close inverted commas

\usepackage{hyperref} % It gives LaTeX the possibility to manage links within the document or to any URL when you compile in PDF.

\usepackage[autostyle]{csquotes} % Package necessary to biblatex
\usepackage[sorting=nyt, style=authoryear]{biblatex} % Advanced bibliography handling. It is the package to use for writing a thesis. bibstyle=authortitle, backend=biber
\addbibresource{activeinference.bib} % File from which take the bibliography 

\usepackage{amsmath,amsfonts,amsthm,bm} % Math packages

\usepackage{comment}

\setlength{\parskip}{0.5em}

%\setcounter{secnumdepth}{0}

\title{Active Inference in continuous time notes}
\author{Federico Maggiore}















\begin{document}
\selectlanguage{english}

\maketitle

\newpage

\tableofcontents

\newpage

\paragraph{\textbf{Summary notation}}
\begin{itemize}

\item $\mathbf x = \lbrace{ x_i \rbrace}_{i=1}^{D}$ environmental variables of the $D$-dimensional space constituting latent or hidden states;

\item $\mathbf{s} = \lbrace{ s_i \rbrace}_{i=1}^{S}$ body sensors input;

\item $\bm \mu = \lbrace \mu_i \rbrace_{i=1}^D$ inner brain state representing the hidden environmental variables $\mathbf x$

\item $P(\mathbf{x},\mathbf{s})$ \emph{Joint density};

\item $P(\mathbf{x}|\mathbf{s})$ \emph{Posterior}; 

\item $P(\mathbf{s}|\mathbf{x})$ \emph{Likelihood};

\item $P(\mathbf x)$ \emph{Prior};

\item $P(\mathbf s)=\int P(\mathbf s|\mathbf x)P(\mathbf x) d\mathbf x $ \emph{marginal likelihood};

\item $Q(\mathbf x)$ \emph{R-density};

\item $F \equiv \int Q(\mathbf x) \ln \frac{Q(\mathbf x)}{P(\mathbf x,\mathbf s)}d\mathbf x $ \emph{Variational Free Energy};

\item $L(\mu,s) \equiv - \ln P(\mu,s)$ \emph{Laplace-encoded energy};

\item $\varepsilon$ \emph{Prediction error};

\item $\Sigma^{-1}$ \emph{Precision}
\end{itemize}

\paragraph{\textbf{References:}} 
\cite{Baltieri2019}, \cite{Buckley2017}, \cite{Bogacz2017},

\section{Introduction}
The goal of an agent is to determine the probability of the hidden states given some sensory inputs:

\begin{equation}
P(\mathbf{x}|\mathbf{s}) = \frac{P(\mathbf{x},\mathbf{s})}{P(\mathbf{s})} = \frac{P(\mathbf{s}|\mathbf{x})P(\mathbf{x})}{P(\mathbf{s})}
\end{equation}
with
\begin{itemize}

\item $P(\mathbf{x},\mathbf{s})$ \emph{joint density}, beliefs about the states assumed to be encoded by the agent;

\item $P(\mathbf{x}|\mathbf{s})$ \emph{Posterior}, i.e. probability of hidden causes $x$ given observed sensory data; 

\item $P(\mathbf{s}|\mathbf{x})$ \emph{Likelihood}, i.e. organism's assumptions about sensory input $\mathbf{s}$ given the hidden causes $\mathbf{x}$;

\item $P(\mathbf x)$ \emph{Prior}, i.e. agent's beliefs about hidden causes \textbf{before} that $\mathbf s$ are received;

\item $P(\mathbf s)=\int P(\mathbf s|\mathbf x)P(\mathbf x) d\mathbf x $ \emph{marginal likelihood}, i.e. normalization factor.

\end{itemize}

For the agent it's not necessary to compute the complete posterior distribution, it has only to find the hidden state -or at least a good approximation- that maximize the posterior, i.e. $\arg \max_{\mathbf{x}} P(\mathbf x|\mathbf s)$.
The problem with the exact Bayesian scheme, is that $P(\mathbf s)$ is often impossible to calculate, and moreover $P(\mathbf x|\mathbf s)$ may not take a standard shape and could not have a summary statistics. 
\section{Predictive Coding}
In signal theory when we have to represent a signal could be convenient, instead of represent it as it is, to code only the information that differ from a given prediction. This is obviously convenient only when is present some predictable redundancy, as it happens in natural images or speech signals.
%A known example is the retina, that heavily pre-processes the visual signal before transmitting it to the cortex.
 
A large group of neuro-computational models are based on the bayesian predictive coding hypothesis, which consists in postulating that an agent, that has to infer the real state of the environment from the sensory input signals, implements this type of coding to tackle these task in combination with approximated bayesian computations. The main differences between these models can be almost always found in the way is postulated that the agent come up with a prediction, i.e. how the exact bayesian calculus is approximated.

\subsection{Rao and Ballard Bayesian Predictive Coding}
The Rao and Ballard Predictive coding model of the visual cortex (\cite{Rao1999})









\newpage
\section{Free Energy Principle (FEP)}


Another biologically plausible technique to approximate the posterior $P(\mathbf x | \mathbf s)$ consist in using an auxiliary distribution $Q(\mathbf x)$ called \emph{recognition density} (\emph{R-density}) that has to be optimized to became a good approximation of the posterior.  

In order to do this the Kullback-Leibler  divergence is minimized:
\begin{equation}
\begin{split}
D_{KL} (\, Q(\mathbf x)\, ||\, P(\mathbf x|\mathbf s)\, ) & = \int Q(\mathbf x) \ln \frac{Q(\mathbf x)}{P(\mathbf x|\mathbf s)} d\mathbf x \\
                                  & = \int Q(\mathbf x) \ln \frac{Q(\mathbf x)P(\mathbf s)}{P(\mathbf x,\mathbf s)} d\mathbf x \\
                                  & = \int Q(\mathbf x) \ln \frac{Q(\mathbf x)}{P(\mathbf x,\mathbf s)} d\mathbf x + \ln P(\mathbf s) \int Q(\mathbf x) d\mathbf x \\
                                  & = F + \ln P(\mathbf s)
\end{split}
\end{equation}

where
\begin{itemize}

\item the term
\begin{equation}
F \equiv \int Q(\mathbf x) \ln \frac{Q(\mathbf x)}{P(\mathbf x,\mathbf s)}d\mathbf x = - \left< \ln P(\mathbf x,\mathbf s) \right>_{Q} + \left< \ln Q(\mathbf x) \right>_{Q}
\end{equation} 
is the \emph{Variational Free Energy} (VFE), a quantity that depends on the R-density and the knowledge about the environment i.e. the joint density $P(\mathbf s, \mathbf x) = P(\mathbf s|\mathbf x)P(\mathbf x)$ that we are assuming the agent has. 
\item $\ln P(\mathbf s)$ is a term independent with respect to the recognition density $Q(\mathbf x)$ ( $\Rightarrow$ minimizing F with respect to $Q(\mathbf x)$ will minimize the $D_{KL}$ )

\end{itemize}

\section{One dimensional case}
\label{sec:1d}
For the sake of simplicity, let's build the framework first in the one dimensional case, to repeat later on all the steps for the multivariate case.

\subsection{Laplace approximation}
\label{sec:laplace}

Often optimizing $F$ for arbitrary $Q(x)$ is particularly complex. Moreover, it is assumed that neural activity parametrise sufficient statistic.
For these reasons, a common approximation is to assume that the R-density take a Gaussian form. 

Let us assume that the R-density $Q(x)$ has a peak at point $\mu$. The Taylor-expansion of the logarithm around this peak is
\begin{equation}
\ln Q(x) \simeq \ln Q(\mu) - \frac{1}{2} \frac{(x-\mu)^2}{\Sigma} 
\end{equation}
with
\begin{equation}
\frac{1}{\Sigma} = - \frac{\partial^{2} }{\partial x^2} \ln Q(x) \bigg\rvert_{x=\mu} 
\end{equation}
Now it is possible to approximate the probability distribution $Q(x)$ with the distribution
\begin{equation}
\mathcal{N}(x;\mu, \Sigma) = \frac{1}{\sqrt{ 2 \pi \Sigma}} \, e^{\frac{(x-\mu)^2}{2 \Sigma}}
\end{equation}
i.e. a Gaussian distribution that has been normalized using the factor $Q(\mu) \sqrt{2 \pi \Sigma }$.\\
Now the VFE can be written as follow
\begin{equation}
\begin{split}
F   & \approx \int \mathcal{N}(x;\mu,\Sigma) (-\frac{1}{2} \ln (2 \pi \Sigma) - \frac{(x-\mu)^2}{2 \Sigma} ) d x - \int \mathcal{N}(x;\mu,\Sigma) \ln P(x,s) d x \\
    & = -\frac{1}{2} \ln (2 \pi \Sigma) - \frac{1}{2 \Sigma} \int \mathcal{N}(x;\mu,\Sigma) (x-\mu)^2  d x - \int \mathcal{N}(x;\mu,\Sigma) \ln P(x,s) d x \\
    & = -\frac{1}{2} \ln (2 \pi \Sigma) - \frac{1}{2} - \int \mathcal{N}(x;\mu,\Sigma) \ln P(x,s) d x
\end{split}
\end{equation}
To end up with an analityc model of the FEP, further simplifications and assumptions are needed to evaluate the last term\footnote{At the end we expand the implications for the interpretation of brain functions due to this issue.}. \\
Let us first assume that the R-density is sharply peaked at its mean value and that $P(x,s)$ is a smooth function of $x$: under these assumptions is possible to consider the integrated function appreciably non-zero only near the peak, and is possible to use a second order Taylor expansion of the $L(x,s) \equiv - \ln P(x,s)$ around $x=\mu$.
\begin{equation}
L(x,s) \approx L(\mu,s) + \left[ \frac{dL(x,s)}{dx} \right]_{x=\mu} (x-\mu) + \frac{1}{2} \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu} (x-\mu)^2 \\
\end{equation}
\begin{center}
$\Downarrow$
\end{center}
\begin{equation}
\begin{split}
- \int \mathcal{N}(x;\mu,\Sigma) \ln P(x,s) d x \approx & \int \mathcal{N}(x;\mu,\Sigma) \Big\lbrace L(\mu,s) + \left[ \frac{dL(x,s)}{dx} \right]_{x=\mu} (x-\mu) \, + \\
													  & + \frac{1}{2} \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu} (x-\mu)^2 \Big\rbrace d x \\
											  		= & \, L(\mu,s) + \left[ \frac{dL(x,s)}{dx} \right]_{x=\mu} \left( \int \mathcal{N}(x;\mu,\Sigma) x \, dx - \mu \right) + \\
											  		& + \frac{1}{2} \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu} \int \mathcal{N}(x;\mu,\Sigma) (x-\mu)^2 \, dx\\
											  		= & \, L(\mu,s) + \frac{1}{2} \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu} \Sigma
\end{split}
\end{equation}
Now is possible to rewrite the variational free energy as
\begin{equation}
F(\mu, \Sigma, s) \approx L(\mu, s) + \frac{1}{2}\left( \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu} \Sigma - \ln (2 \pi \Sigma) -1 \right)
\end{equation}
with $L(\mu, s)$ said \emph{Laplace-encoded energy}, and the variational free energy written as a function and not anymore as a functional.

Since the goal is to minimize the Kullback-Leibler divergence trough the minimization of the VFE, is possible to simplify further removing the $\Sigma$ dependency taking the derivative with respect this and imposing $\frac{dF}{d\Sigma}=0$
\begin{equation}
\frac{dF}{d\Sigma}= \frac{1}{2} \left( \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu} - \frac{1}{\Sigma} \right) = 0
\end{equation}
\begin{equation}
\Rightarrow \Sigma = \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu}^{-1} \equiv \Sigma^{\ast}
\end{equation}

The final form of the VFE is then
\begin{equation}
F \approx L(\mu,s) - \frac{1}{2} \ln \left( 2 \pi \Sigma^{\ast} \right) \, ,
\end{equation}
that can be further simplified if the $L$ function has a second order polynomial form\footnote{as we will see in Sec.(\ref{sec:G})}, that implies that the second-order derivative of $L$ with respect to $x$ results in a constant factor that is useless and can be ignored\footnote{ let us remind that the final goal is to minimize $F$ with respect to $x$}, leading to 
\begin{equation}
F \approx L(\mu,s)
\label{eqn:encoded_F}
\end{equation}


\paragraph{About Laplace Approximation}
Carrying on a theory based on an approximation that leads to Eq.(\ref{eqn:encoded_F}), means to assume that the brain represents only the most likely environmental cause of sensory data and nothing else about the distribution. However, as we're going to see, the uncertainties are encoded directly in the form of the joint density.



\subsection{Building the generative model and VFE minimisation}
\label{sec:G}
Thanks to the Laplace approximation, we've been able to write the VFE in terms of the Laplace-encoded energy $L(\mu,s)$, that in turn depends on the joint density $P(x, s)$. In this function are encoded the brain beliefs about the environmental causes of the sensory input and the beliefs \emph{a priori} about environmental states.

Therefore, what needs to be built is a \emph{generative model}, that is a model in which is encoded how the brain believe the world works and where all the hypothesis about the agent's behaviour are formalized.

\subsubsection{Static Model}

Let us consider a simple case of an agent that believes in an environment with hidden state $ x$ that stimulates a sensory channel $ s$. As we've seen in Sec.(\ref{sec:laplace}), the brain will represent the environment only through the inner state $\mu$, and what remains to do is to explicit the mapping between brain states and sensory data that will allows to make explicit the joint density.

Let us assume that the agent believes its sensory input are generated by
\begin{equation}
s = g(x) + \mathcal{N}(s;0,\Sigma_s) \, ,
\label{eqn:input}
\end{equation}
with $g$ generic function that expresses the relation between states and sensory input, to which is summed a noise represented by the normal distribution with zero mean and variance $\Sigma_s$. This assumption means that we can write
\begin{equation}
P(s|x) = \mathcal{N}(s;g(x),\Sigma_s) = \frac{1}{\sqrt{ 2 \pi \Sigma_{s}}} \, e^{\frac{(s-g(x))^2}{2 \Sigma_{s}}} \, .
\end{equation}
Moreover let us also assume that the agent also has a prior knowledge regarding the environmental state given by $\bar{\mu}$ that is linked with the inner state through
\begin{equation}
x = \bar{x} + \mathcal{N}(x;0,\Sigma_{x})
\end{equation}
\begin{equation}
\Rightarrow P(x) = \mathcal{N}(x;\bar{x},\Sigma_{x}) =\frac{1}{\sqrt{ 2 \pi \Sigma_{x}}} \, e^{\frac{(x-\bar{x})^2}{2 \Sigma_{x}}} \, .
\end{equation}

Now that we have specified a likelihood and a prior, is possible to determine the joint density
\begin{equation}
P(x,s) = P(s | x) P(x)
\end{equation}
and consequently the Laplace-encoded energy
\begin{equation}
\begin{split}
L(\mu,s) & = - \ln P(s|\mu) - \ln P(\mu)  \\
		 & = \frac{1}{2} \ln (2 \pi \Sigma_{s}) + \frac{(s-\mu)^2}{2 \Sigma_{s}} + \frac{1}{2} \ln (2 \pi \Sigma_{x}) + \frac{(\mu-\bar{x})^2}{2 \Sigma_{x}} \\
		 & = \frac{\varepsilon_{s}^2}{2 \Sigma_{s}} + \frac{\varepsilon_{\mu}^2}{2 \Sigma_{x}} + \frac{1}{2} \ln \left( \Sigma_{s} \Sigma_{x} \right) + \ln (2 \pi) \, ,
\end{split}
\end{equation}
where the $\varepsilon$ terms are said \emph{prediction errors} and measure the discrepancy respectively between the actual sensory data $s$ and the outcome of its prediction $g(x)|_{x=\mu}$ and between $\mu$ itself and its prior expectation $\bar{x}$. Therefore the former $\varepsilon_{s}$ describes sensory prediction errors, the latter $\varepsilon_{\mu}$ model prediction errors (i.e. how brain states deviate from their expectation) and each one is weighted with the the corresponding inverse of the variance $\Sigma_{s}^{-1}$ and $\Sigma_{x}^{-1}$ (which are often said \emph{precisions}). \\
As said at the end of Sec.(\ref{sec:laplace}), since the $L$ function has a quadratic form is possible to ignore all the terms apart from the following
\begin{equation}
F \approx \frac{1}{2} \left[\frac{\varepsilon_{s}^2}{\Sigma_{s}} + \frac{\varepsilon_{x}^2}{ \Sigma_{x}} + \ln \left( \Sigma_{s} \Sigma_{x} \right) \right] \, .
\end{equation}

The last thing that remains to do is to find a biologically plausible mechanism to minimise VFE.\\
In the Free Energy Principle framework, it is proposed that the innate dynamics of the neural activity evolves in such a way that it implement a gradient descent scheme on the VFE. \\
In particular, in the static model case, a brain state $\mu$ is updated between two (internal) sequential steps $t$ and $t+dt$ as
\begin{equation}
\mu^{t+dt} = \mu^{t} - k \cdot \nabla_{\mu} F = \mu^{t} - k \cdot \nabla_{\mu} L(\mu, s) \, ,
\end{equation}
with $k$ learning rate parameter that has to be tuned and $\nabla_{\mu} L(\mu, s)$ that goes to zero when a minimum of the $L$ function is reached.

\subsubsection{Dynamic Model}
Let's formulate a possible implementation of inference in a dynamically changing environment.

It is clear that, in order to describe a dynamical system, the environmental variables describing the world must be at least two: $x$ and his first order derivative with respect to time $\frac{dx}{dt} \equiv \dot{x}$. 
For this reason the agent, together with a prior knowledge regarding the environmental state $x$, must model the environmental dynamic.

Since typically in a dynamic case the variable $x$ is allowed to vary without any boundary, a flat prior is usually associated to it (i.e. zero prior knowledge), while its dynamic is described using a Langevin-type equation as
\begin{equation}
\frac{d x(t)}{dt} = f(x) + z_{\dot{x}}(t) \, .
\label{eqn:x}
\end{equation}

As the static case, is assumed that the agent believes its sensory input are generated in a similar manner with respect to Eq.(\ref{eqn:input}), in particular
\begin{equation}
s(x,t) = g(x) + z_s(t) \, .
\label{eqn:s}
\end{equation}

Both $z_s$ and $z_{\dot{x}}$ are again terms representing gaussian noise with zero mean and variances respectively equal to $\Sigma_s$ and $\Sigma_{\dot{x}}$.


\begin{equation}
P(x, \dot{x}, s) = P(s|x) P(\dot{x}|x) P(x) = C \cdot \mathcal{N}(s;x,\Sigma_s) \mathcal{N}(\dot{x};x,\Sigma_{\dot{x}})
\end{equation}

\begin{equation}
\begin{split}
L(\dot{\mu}, \mu, s) &=  \frac{(s-g(x))^2}{2 \Sigma_{s}} + \frac{1}{2} \ln (2 \pi \Sigma_{s})+ \frac{(\dot{\mu}-f(\mu))^2}{2 \Sigma_{\dot{\mu}}} + \frac{1}{2} \ln (2 \pi \Sigma_{\dot{\mu}}) + C \\
	&= \frac{\varepsilon_{s}^2}{2 \Sigma_{s}} + \frac{\varepsilon_{\dot{\mu}}^2}{2 \Sigma_{\dot{\mu}}} + \frac{1}{2} \ln (\Sigma_{s}\Sigma_{\dot{\mu}}) + \ln(2 \pi) + C
\end{split}
\end{equation}


\begin{equation}
\mu^{t+dt} = \mu^{t} - k \cdot \nabla_{\mu} F = \mu^{t} - k \cdot \nabla_{\mu} L(\dot{\mu}, \mu, s) \, ,
\end{equation}
\begin{equation}
\dot{\mu}^{t+dt} = \dot{\mu}^{t} - k \cdot \nabla_{\dot{\mu}} F = \dot{\mu}^{t} - k \cdot \nabla_{\dot{\mu}} L(\dot{\mu}, \mu, s) \, ,
\end{equation}


\begin{equation}
\mu^{t+dt} = \mu^{t} + \dot{\mu} dt - k \cdot \nabla_{\mu} F = \mu^{t} + \dot{\mu} dt - k \cdot \nabla_{\mu} L(\dot{\mu}, \mu, s) \, ,
\end{equation}
\begin{equation}
\dot{\mu}^{t+dt} = \dot{\mu}^{t} - k \cdot \nabla_{\dot{\mu}} F = \dot{\mu}^{t} - k \cdot \nabla_{\dot{\mu}} L(\dot{\mu}, \mu, s) \, ,
\end{equation}


\newpage

Using a \emph{generalised state-space model}, in which the state of a dynamical system is represented in terms of increasingly higher order derivative of its state variables, in combination with local linearity approximation on higher orders of motion\footnote{Without this approximation the model would scale-up very quickly becoming complicated and unwieldy fairly quickly. This approximation becomes exact when $f$ and $g$ are linear.} suppressing non-linear terms in the partial derivatives, is possible to obtain 
\begin{equation}
  \begin{split}
    s &= g(x) + z_{s}(t) \\
    s' &= \frac{\partial g}{\partial x}x' + z_{s}' \\
    s'' & \simeq \frac{\partial g}{\partial x}x'' + z_{s}''\\
    				& \qquad \vdots
  \end{split}
  \qquad
  \begin{split}
    x' &= f(x) + z_{x}(t) \\
    x'' &= \frac{\partial f}{\partial x}x' + z_{x}' \\
    x''' & \simeq \frac{\partial f}{\partial x}x'' + z_{x}''\\
    				& \qquad \vdots
  \end{split}
\label{eqn:hdm}
\end{equation}
where we have used the notation
\begin{equation}
s' = \frac{ds}{dt} \, , \qquad x'=\frac{dx}{dt} \, , \qquad s''= \frac{d^2 s}{dt^2} \, , \qquad x''= \frac{d^2 x}{dt^2} \, , \quad \dots
\end{equation}
and where $z_{s},z_{s}',z_{s}'',\dots, z_{x},z_{x}',z_{x}'',\dots$ are the noise sources at each dynamic order.
Considering the previous linear approximation as equalities, Eq.(\ref{eqn:hdm}) can be expressed in the more compact form
\begin{equation}
\tilde{s} = g(\tilde{x}) + \tilde{z_{s}} \quad , \quad \tilde{x}' = f(\tilde{x}) + \tilde{z_{x}}
\end{equation}
using the notation 
\begin{equation}
\tilde{s} = (s, s', s'', \dots) \equiv (s_{[0]}, s_{[1]}, s_{[2]}, \dots)  \quad , \quad \tilde{x} = (x, x ', x '', \dots) \equiv (x_{[0]}, x_{[1]}, x_{[2]}, \dots)\, ,
\end{equation}
where
\begin{equation}
s_{[n]} \equiv \frac{d^n}{dt^n}s = s_{[n-1]}' \quad , \quad x_{[n]} \equiv \frac{d^n}{dt^n}x = x_{[n-1]}' \, ,
\end{equation}
\begin{equation}
\tilde{x}' \equiv D \tilde{x} = \frac{d}{dt}(x,x',x'',\dots)=(x',x'',x''',\dots)\equiv(x_{[1]}, x_{[2]}, x_{[3]}, \dots)
\end{equation}
and
\begin{equation}
\tilde{g} \equiv (g_{[0]}, g_{[1]}, g_{[2]}, \dots) \quad , \quad \tilde{f} \equiv (f_{[0]}, f_{[1]}, f_{[2]}, \dots)
\end{equation}
with
\begin{equation}
\begin{split}
g_{[0]} & \equiv g(x) \\
g_{[1]} & \equiv \frac{\partial g}{\partial x} x_{[0]} \\
& \qquad \vdots \\
g_{[n]} & \equiv \frac{\partial g}{\partial x} x_{[n]} \\
& \qquad \vdots
\end{split}
\qquad
\begin{split}
f_{[0]} & \equiv f(x) \\
f_{[1]} & \equiv \frac{\partial f}{\partial x} x \\
& \qquad \vdots \\
f_{[n]} & \equiv \frac{\partial f}{\partial x} x_{[n]} \\
& \qquad \vdots
\end{split}
\end{equation}

\paragraph{Considerations about noise sources}
the stochastic terms $z_{s}(t)$ and $z_{x}(t)$ in Eq.(\ref{eqn:s}) and Eq.(\ref{eqn:x}) are analytic and form stochastic equations based on Stratonovich calculus, with well defined covariances of $\tilde{z_{s}}(t) = (z_{s[0]}, z_{s[1]}, z_{s[2]}, \dots)$ and $\tilde{z_{x}}(t) = (z_{x[0]}, z_{x[1]}, z_{x[2]}, \dots)$. This property is important to define a non-Markovian process, because in Ito's formulation, based on Wiener noise, the autocorrelation functions can be seen as strictly equal to delta functions representing perfect white noise not existing in real world. \textbf{Da approfondire}\\
A common approximation that brings to a really simple form of the joint density $P(x,s)$ is the one in which the covariances between dynamical orders are assumed equal to zero, i.e. independent noise sources, that leads to 
\begin{equation}
\begin{split}
P(\tilde{s} | \tilde{x}) & = P(s_{[0]}, s_{[1]}, s_{[2]}, \dots | x_{[0]}, x_{[1]}, x_{[2]}, \dots) = \prod_{n=0}^{\infty} P(s_{[n]} | x_{[n]}) \\
P(\tilde{x}) & = P(x_{[0]}, x_{[1]}, x_{[2]}, \dots) = P(x_{[0]}) \prod_{n=0}^{\infty} P(x_{[n+1]} | x_{[n]}) \\
\end{split}
\end{equation}
Assuming in addition that at all dynamics orders $z_{s[n]}$ and $z_{x[n]}$ have a Gaussian form we can write
\begin{equation}
P(s_{[n]} | x_{[n]}) = \mathcal{N}(s_{[n]};g_{[n]},\Sigma_{s[n]})
	=\frac{1}{\sqrt{2\pi \Sigma_{s[n]}}} \, e^{-\frac{(s_{[n]}-g_{[n]})^2}{2 \Sigma_{s[n]}}}
\end{equation}
and
\begin{equation}
P(x_{[n+1]} | x_{[n]}) = \mathcal{N}(x_{[n+1]};f_{[n]},\Sigma_{x[n]})
	=\frac{1}{\sqrt{2\pi \Sigma_{x[n]}}} \, e^{-\frac{(x_{[n+1]}-f_{[n]})^2}{2 \Sigma_{x[n]}}} \, .
\end{equation}
In dynamical systems the $x_{[0]}$ prior at each time step is usually omitted because the agent does not explicitly desire any $x_{[0]}$, giving it a flat prior (i.e. infinite variance) that eliminates the corresponding term from the VFE. 

In conclusion the joint density can be write as
\begin{equation}
P(\tilde{x}, \tilde{s}) = \prod_{n=0}^{\infty} P(s_{[n]} | x_{[n]}) P(x_{[n+1]} | x_{[n]})
\end{equation}
that, inserted in the Laplace-encoded energy lead to 
\begin{equation}
\begin{split}
L(\tilde{\mu}, \tilde{s}) &= \sum_{n=0}^{\infty} \left[ \frac{(s_{[n]}-g_{[n]})^2}{2 \Sigma_{s_{[n]}}} + \frac{1}{2} \ln (2 \pi \Sigma_{s_{[n]}}) \right] + \left[ \frac{(\mu_{[n+1]}-f_{[n]})^2}{2 \Sigma_{\mu_{[n]}}} + \frac{1}{2} \ln (2 \pi \Sigma_{\mu_{[n]}}) \right] \\
	&= \sum_{n=0}^{\infty} \frac{\varepsilon_{s_{[n]}}^2}{2 \Sigma_{s_{[n]}}} + \frac{\varepsilon_{\mu_{[n]}}^2}{2 \Sigma_{\mu_{[n]}}} + \frac{1}{2} \ln (\Sigma_{s_{[n]}}\Sigma_{\mu_{[n]}}) + \ln(2 \pi)\\
\end{split}
\end{equation}
with $\varepsilon_{s_{[n]}} \equiv s_{[n]}-g_{[n]}$ and $\varepsilon_{\mu_{[n]}} \equiv \mu_{[n+1]}-f_{[n]}$ $n$th component of $\tilde{\varepsilon_{s}}$ and $\tilde{\varepsilon_{\mu}}$ that, as in the static case, are said \emph{prediction errors}, which encode respectively the discrepancy between sensory data $\tilde{s}$ and its prediction $\tilde{g}$ and the difference between the expected higher-order output $\tilde{\mu}'$ and its generation $\tilde{f}$.

Usually only dynamics up to a finite order $n_{max}$ are considered, and this is done by setting 
\begin{equation}
x_{[n_{max}+1]} = z_{x [n_{max}]}
\end{equation}
with $\Sigma_{x_{[n_{max}]}}$ large, so that the corresponding error term will be close to zero and can be ignored in the Laplace-encoded energy, meaning that the order below is unconstrained and free to change in a way that best fits the incoming sensory data.\\
Finally, since also in this case the $L$ function has a quadratic form, when writing down the VFE is possible to ignore all the terms apart from
\begin{equation}
F \approx \frac{1}{2} \, \sum_{n=0}^{n_{max}} \left[ \frac{\varepsilon_{s_{[n]}}^2}{ \Sigma_{s_{[n]}}} + \frac{\varepsilon_{\mu_{[n]}}^2}{\Sigma_{\mu_{[n]}}} + \ln (\Sigma_{s_{[n]}}\Sigma_{\mu_{[n]}}) \right]
\end{equation}


\subsection{VFE minimisation}
In Sec.(\ref{sec:laplace}) we have expressed, in an approximated form, the Variational Free Energy in terms of the Laplace-encoded Energy, which depends on joint density $P(\mathbf x, \mathbf s)$, that we have seen how to build starting from the generative model in Sec.(\ref{sec:G}). Now we are going to see a possible implementation of a biologically plausible mechanism to minimise VFE.

In the Free Energy Principle framework, it is proposed that the innate dynamics of the neural activity evolves in such a way that it implement a gradient descent scheme on the VFE. \\
In particular, in the static model case, a brain state $\mu$ is updated between two (internal) sequential steps $t$ and $t+dt$ as
\begin{equation}
\mu^{t+dt} = \mu^{t} - k \cdot \nabla_{\mu} F = \mu^{t} - k \cdot \nabla_{\mu} L(\mu, s) \, ,
\end{equation}
with $k$ learning rate parameter that has to be tuned and $\nabla_{\mu} L(\mu, s)$ that goes to zero when a minimum of the $L$ function is reached.\\
In the dynamic case instead,  





\newpage
\section{Multivariate case}
Now that we have seen how to organize an active inference framework in the one dimensional case, let's see how to scale up the dimensions of our system remembering that $\mathbf x = \lbrace x_i \rbrace_{i=1}^D$, $\mathbf s = \lbrace s_i \rbrace_{i=1}^S$ and $\bm \mu = \lbrace \mu_i \rbrace_{i=1}^D$.

Let's recall that the goal is to minimize the Kullback-Leibler  divergence
\begin{equation}
D_{KL} (\, Q(\mathbf x)\, ||\, P(\mathbf x|\mathbf s)\, )  =  F + \ln P(\mathbf s) = - \left< \ln P(\mathbf x,\mathbf s) \right>_{Q} + \left< \ln Q(\mathbf x) \right>_{Q} 
\end{equation}
that, since $P(\mathbf s)$ is independent with respect to the recognition density $Q(\mathbf x)$, is equivalent to minimize the Variational Free Energy
\begin{equation}
F = \int Q(\mathbf x) \ln Q(\mathbf x) d\mathbf x - \int Q(\mathbf x) \ln P(\mathbf x,\mathbf s) d\mathbf x
\end{equation}


\subsection{Laplace approximation}
As done before, let's start assuming that neural activity parametrise sufficient statistic, in particular let's approximate the R-density $Q(\mathbf x)$ with a multivariate Gaussian over the D-dimensional space $\mathbf x$, with peak at $\boldsymbol \mu$, and let's do the same procedure done in Sec.(\ref{sec:laplace}).

\begin{equation}
\ln Q(\mathbf x) \simeq \ln Q(\boldsymbol{\mu}) - \frac{1}{2} (\mathbf x-\boldsymbol \mu)^T \boldsymbol{\hat{\Sigma}}^{-1} (\mathbf x-\boldsymbol \mu)
\end{equation}
with
\begin{equation}
\left[ \boldsymbol{\hat{\Sigma}}^{-1} \right]_{i,j} = - \frac{\partial^{2} }{\partial x_i \partial x_j} \ln Q(\mathbf x) \bigg\rvert_{\mathbf x=\boldsymbol \mu} 
\end{equation}
Now let us approximate $Q(\mathbf x)$ with the multivariate Gaussian distribution
\begin{equation}
\mathcal{N}(\mathbf x;\boldsymbol \mu, \boldsymbol{\hat{\Sigma}}) = \frac{1}{\sqrt{( 2 \pi)^{D} \det \boldsymbol{\hat{\Sigma}}}} \, e^{ - \frac{1}{2} (\mathbf x-\boldsymbol \mu)^T \boldsymbol{\hat{\Sigma}}^{-1} (\mathbf x-\boldsymbol \mu)}
\end{equation}
and rewrite the VFE as follow:

\begin{equation}
\label{eqn:f1}
\begin{split}
F \approx & \int \mathcal{N}(\mathbf x; \bm \mu, \bm{\hat{\Sigma}}) \left[ -\frac{1}{2} \ln \left( (2 \pi)^{D} \det \bm{\hat{\Sigma}} \right) - \frac{1}{2} (\mathbf x-\boldsymbol \mu)^T \boldsymbol{\hat{\Sigma}}^{-1} (\mathbf x-\boldsymbol \mu) \right] d \mathbf x \, + \\
	& - \int \mathcal{N}(\mathbf x;\bm \mu,\bm{\hat{\Sigma}}) \ln P(\mathbf x,\mathbf s) d \mathbf x \\
    = & -\frac{1}{2} \ln \left( (2 \pi)^{D} \det \bm{\hat{\Sigma}} \right) - \int \mathcal{N}(\mathbf x; \bm \mu, \bm{\hat{\Sigma}}) \left[ \frac{1}{2} (\mathbf x-\boldsymbol \mu)^T \boldsymbol{\hat{\Sigma}}^{-1} (\mathbf x-\boldsymbol \mu) \right] d \mathbf x \, + \\
    & - \int \mathcal{N}(\mathbf x;\bm \mu,\bm{\hat{\Sigma}}) \ln P(\mathbf x,\mathbf s) d \mathbf x 
\end{split}
\end{equation}

Let us focus on the second term making as first thing the change of variables with unitary Jacobian $\mathbf y = \mathbf x - \bm \mu$. 
\begin{equation}
\int \mathcal{N}(\mathbf x; \bm \mu, \bm{\hat{\Sigma}}) \left[ \frac{1}{2} (\mathbf x-\boldsymbol \mu)^T \boldsymbol{\hat{\Sigma}}^{-1} (\mathbf x-\boldsymbol \mu) \right] d \mathbf x = 
\int \mathcal{N}(\mathbf y; \bm 0, \bm{\hat{\Sigma}}) \left[ \frac{1}{2} \mathbf y^T \boldsymbol{\hat{\Sigma}}^{-1} \mathbf y \right] d \mathbf y
\end{equation}
After that, since $\bm{\hat{\Sigma}} ^{-1}$ is a symmetric and real matrix, the spectral theorem guarantees the existence of an orthonormal matrix $\mathbf U$ such that $ \mathbf U^{T} \mathbf{\hat{\Sigma}}^{-1} \mathbf U = \bm \Lambda$, with $\bm \Lambda$ diagonal matrix containing the eigenvalues $\lbrace \lambda_i \rbrace_{i=1}^D$ of $\bm{\hat{\Sigma}}^{-1}$ and $\mathbf U$ containing as columns the eigenvectors of $\bm{\hat{\Sigma}}^{-1}$, so adding the identity matrix $\mathbf I = \mathbf U \mathbf U^{T}$ we get
\begin{equation}
\mathbf y^{T} \bm{\hat{\Sigma}}^{-1} \mathbf y = \mathbf y^{T} \mathbf U \mathbf U^{T} \bm{\hat{\Sigma}}^{-1} \mathbf U \mathbf U^{T} \mathbf y = \mathbf z^{T} \bm \Lambda \mathbf z = \sum_{i=1}^{D} \lambda_i z_i^2  \, ,
\end{equation}
where $\mathbf z = \mathbf U^{T} \mathbf y$ is the $\mathbf y$ representation in the orthonormal basis given by the eigenvectors of $\bm \Sigma^{-1}$. Therefore moving to the variable $\mathbf z$ (the Jacobian of this change of variables is unitary too), we obtain
\begin{equation}
\frac{1}{\sqrt{( 2 \pi)^{D} \det \boldsymbol{\hat{\Sigma}}}} \, \int \frac{1}{2} \left[ \sum_{i=1}^{D} \lambda_i z_i^2 \right] \, e^{ - \frac{1}{2} \sum_{i=1}^D \lambda_i z_i^2 } d \mathbf z = \frac{1}{2} \sqrt{\frac{\prod_{i=1}^D \lambda_i}{( 2 \pi)^{D}}} \, \sum_{i=1}^{D} \int \lambda_i z_i^2 \, e^{ - \frac{1}{2} \sum_{i=1}^{D} \lambda_i z_i^2} d z_{i} 
\end{equation}
indicating with $\mathbf z_{\not{ \, i}} \equiv \lbrace z_j \rbrace_{j \neq i}$
\begin{multline}
\frac{1}{2} \sqrt{\frac{\prod_{i=1}^D \lambda_i}{( 2 \pi)^{D}}} \, \sum_{i=1}^{D} \int \lambda_i z_i^2 \, e^{ - \frac{1}{2} \lambda_i z_i^2} d z_i \int e^{\frac{1}{2} \sum_{j \neq i} \lambda_i z_j^2} d \mathbf z_{\not{\, i}} = \\
= \frac{1}{2} \sqrt{\frac{\prod_{i=1}^D \lambda_i}{( 2 \pi)^{D}}} \, \sum_{i=1}^{D} \lambda_i \sqrt{\frac{2 \pi}{\lambda_i}} \frac{1}{\lambda_i} \, \sqrt{\frac{(2\pi)^{D-1}}{\prod_{\not{\, i}} \lambda_{\not{\, i}}}} = \frac{1}{2} \sqrt{\frac{\prod_{i=1}^D \lambda_i}{( 2 \pi)^{D}}} \, \sum_{i=1}^{D} \sqrt{\frac{(2 \pi)^D}{\prod_{i=1}^D \lambda_i}}  = \frac{D}{2}
\end{multline}
In conclusion then
\begin{gather}
\int \mathcal{N}(\mathbf x; \bm \mu, \bm{\hat{\Sigma}}) \left[ \frac{1}{2} (\mathbf x-\boldsymbol \mu)^T \boldsymbol{\hat{\Sigma}}^{-1} (\mathbf x-\boldsymbol \mu) \right] d \mathbf x = \frac{D}{2} \\
\Rightarrow F \approx -\frac{1}{2} \ln \left( (2 \pi)^{D} \det \bm{\hat{\Sigma}} \right) - \frac{D}{2} - \int \mathcal{N}(\mathbf x;\bm \mu,\bm{\hat{\Sigma}}) \ln P(\mathbf x,\mathbf s) d \mathbf x 
\end{gather}

Let us now evaluate the last term. As done before, let us assume that the R-density is sharply peaked at its mean value (i.e. both variance of and covariances between variables are small) and that $P(\mathbf x, \mathbf s)$ is a smooth function of $\mathbf x$: under these assumptions is possible to consider the integrated function appreciably non-zero only near the peak, and is possible to use a second order Taylor expansion of the $L(\mathbf x,\mathbf s) \equiv - \ln P(\mathbf x,\mathbf s)$ around $\mathbf x=\bm \mu$.
\begin{equation}
L(\mathbf x, \mathbf s) \approx L(\bm \mu, \mathbf s) +  (\mathbf x-\bm \mu)^T \left[ \nabla L(\mathbf x,\mathbf s) \right]_{\mathbf x=\bm \mu} + \frac{1}{2} (\mathbf x-\bm \mu)^T \left[ \nabla^2 L(\mathbf x, \mathbf s) \right]_{x=\mu} (\mathbf x-\bm \mu)
\end{equation}
\begin{center}
$\Downarrow$
\end{center}
\begin{equation}
\begin{split}
- \int \mathcal{N}(\mathbf x;\bm \mu,\bm{\hat{\Sigma}}) \ln P(\mathbf x,\mathbf s) d \mathbf x \approx & \int \mathcal{N}(\mathbf x;\bm \mu,\bm{\hat{\Sigma}}) \Big\lbrace L(\bm \mu, \mathbf s) +  (\mathbf x-\bm \mu)^T \left[ \nabla L(\mathbf x,\mathbf s) \right]_{\mathbf x=\bm \mu} + \\
	& + \frac{1}{2} (\mathbf x-\bm \mu)^T \left[ \nabla^2 L(\mathbf x, \mathbf s) \right]_{x=\mu} (\mathbf x-\bm \mu) \Big\rbrace d \mathbf x \\
	= & \, L(\bm \mu, \mathbf s) + \left( \int \mathcal{N}(\mathbf x; \bm \mu,\bm{\hat{\Sigma}}) \mathbf x^T \, d \mathbf x - \bm \mu^T \right) \left[ \nabla L(\mathbf x,\mathbf s) \right]_{\mathbf x=\bm \mu} \\
	& + \frac{1}{2} \int \mathcal{N}(\mathbf x;\bm \mu,\bm{\hat{\Sigma}}) (\mathbf x-\bm \mu)^T \left[ \nabla^2 L(\mathbf x, \mathbf s) \right]_{x=\mu} (\mathbf x-\bm \mu) \, d \mathbf x \\
\stackrel{?}{=} &\, L(\bm \mu,\mathbf s) + \frac{D}{2} \left[ \nabla^2 L(\mathbf x, \mathbf s) \right]_{x=\mu} \bm{\hat{\Sigma}}
\end{split}
\end{equation}
\begin{comment}
Now is possible to rewrite the variational free energy as
\begin{equation}
F(\bm \mu, \bm{\hat{\Sigma}}, \mathbf s) \approx L(\bm \mu,\mathbf s) + \frac{1}{2}\left( D \left[ \nabla^2 L(\mathbf x, \mathbf s) \right]_{x=\mu} \bm - \ln \left( (2 \pi)^{D} \det \bm \Sigma \right) - D \right)
\end{equation}
with again $L(\bm \mu,\mathbf s)$ as \emph{Laplace-encoded energy}, and the variational free energy written as a function and not anymore as a functional.

Since the goal is to minimize the Kullback-Leibler divergence trough the minimization of the VFE, is possible to simplify further removing the $\Sigma$ dependency taking the derivative with respect this and imposing $\frac{dF}{d\Sigma}=0$
\begin{equation}
\frac{dF}{d\Sigma}= \frac{1}{2} \left( \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu} - \frac{1}{\Sigma} \right) = 0
\end{equation}
\begin{equation}
\Rightarrow \Sigma = \left[ \frac{d^2L(x,s)}{dx^2} \right]_{x=\mu}^{-1} \equiv \Sigma^{\ast}
\end{equation}

The final form of the VFE is then
\begin{equation}
F \approx L(\mu,s) - \frac{1}{2} \ln \left( 2 \pi \Sigma^{\ast} \right)
\end{equation}
\end{comment}

\begin{equation}
\vdots \notag
\end{equation}

\begin{equation}
F \approx L(\bm \mu,\mathbf s)
\end{equation}






\subsection{Building the generative model}
Now that we've expressed the VFE in terms of the Laplace-encoded Energy $L(\bm \mu , \mathbf s)$, let's develop the generative model through the building of the joint density $P(\mathbf x,\mathbf s)$.

\subsubsection{Static Model}
Here the agent believes in an environment with hidden state $\mathbf x$ that stimulates a sensory channel $\mathbf s$ and that it builds the joint density using a likelihood and a prior.

The prior is built from the expectations 
\begin{equation}
\mathbf x = \bm \bar{\bm \mu} + \mathbf z_{\bar{\bm \mu}} \, ,
\end{equation}
where $\mathbf z_{\bar{\bm \mu}}$ is a $D$-dimensional vector describing correlated noise with zero mean and covariance matrix $\bm{\hat{\Sigma}}_{\bar{\bm \mu}}$ that allows us to write
\begin{equation}
P(\mathbf x) = \mathcal{N}(\mathbf x; \bar{\bm \mu}, \bm{\hat{\Sigma}}_{\bar{\bm \mu}}) = \frac{1}{\sqrt{( 2 \pi)^{D} \det \boldsymbol{\hat{\Sigma}}_{\bar{\bm \mu}}}} \, e^{ - \frac{1}{2} (\mathbf x - \bm \bar{\bm \mu})^T \bm{\hat{\Sigma}}_{\bar{\bm \mu}}^{-1} (\mathbf x - \bm \bar{\bm \mu})} \, .
\end{equation}
Similarly the sensory inputs are assumed to be generated by
\begin{equation}
\mathbf s = \bm g(\mathbf x) + \mathbf z_{\mathbf s} \, ,
\end{equation}
with again $\mathbf z_{\mathbf s}$ $S$-dimensional vector describing correlated noise with zero mean and covariance matrix $\bm{\hat{\Sigma}}_{\mathbf s}$ that brings to the likelihood
\begin{equation}
P(\mathbf s | \mathbf x) = \mathcal{N}(\mathbf s; \bm g(\mathbf x), \bm{\hat{\Sigma}}_{\mathbf s}) = \frac{1}{\sqrt{( 2 \pi)^{S} \det \boldsymbol{\hat{\Sigma}}_{\mathbf s}}} \, e^{ - \frac{1}{2} \left(\mathbf s - \bm g(\mathbf x) \right)^T \bm{\hat{\Sigma}}_{\mathbf s}^{-1} \left(\mathbf s - \bm g(\mathbf x) \right)} \, .
\end{equation}

Now is possible to write the Laplace-encoded energy
\begin{equation}
\begin{split}
L(\bm \mu, \mathbf s) = & \, \frac{1}{2} \ln \left( (2 \pi)^S \det \bm{\hat{\Sigma}}_{s} \right) + \frac{1}{2} \left(\mathbf s - \bm g(\bm \mu) \right)^T \bm{\hat{\Sigma}}_{\mathbf s}^{-1} \left(\mathbf s - \bm g(\bm \mu) \right) \\
			& + \frac{1}{2} \ln \left( (2 \pi)^D \det \bm{\hat{\Sigma}}_{\bar{\bm \mu}} \right) + \frac{1}{2} (\bm \mu - \bm \bar{\bm \mu})^T \bm{\hat{\Sigma}}_{\bar{\bm \mu}}^{-1} (\bm \mu - \bm \bar{\bm \mu}) \\
		=	& \, \frac{1}{2} \bm{\varepsilon_{\mathbf s}}^T \bm{\hat{\Sigma}}_{\mathbf s}^{-1} \bm{\varepsilon_{\mathbf s}} + \frac{1}{2} \bm{\varepsilon_{\bm \mu}}^T \bm{\hat{\Sigma}}_{\bar{\bm \mu}}^{-1} \bm{\varepsilon_{\bm \mu}} \\
			& + \frac{1}{2} \ln \left( \det \bm{\hat{\Sigma}}_{\bar{\bm \mu}} \det \bm{\hat{\Sigma}}_{s} \right) + \frac{1}{2} \ln \left( (2 \pi)^D (2 \pi)^S  \right) \, ,
\end{split}
\end{equation}
where again we have introduced the prediction errors $\bm{\varepsilon_{\mathbf s}} \equiv \mathbf s - \bm g(\bm \mu)$ and $\bm{\varepsilon_{\bm \mu}} \equiv \bm \mu - \bm \bar{\bm \mu}$ and we're able to write the VFE approximated form getting rid of all constants and the term depending on $\bm{\hat{\Sigma}}^{\star}$ :
\begin{equation}
F \approx \frac{1}{2} \bm{\varepsilon_{\mathbf s}}^T \bm{\hat{\Sigma}}_{\mathbf s}^{-1} \bm{\varepsilon_{\mathbf s}} + \frac{1}{2} \bm{\varepsilon_{\bm \mu}}^T \bm{\hat{\Sigma}}_{\bar{\bm \mu}}^{-1} \bm{\varepsilon_{\bm \mu}} + \frac{1}{2} \ln \left( \det \bm{\hat{\Sigma}}_{\bar{\bm \mu}} \det \bm{\hat{\Sigma}}_{s} \right) \, .
\end{equation}

Often, the VFE is further simplified making another assumption, that is the \emph{mean field approximation}, which implies statistical independence between environmental variables and between sensory inputs\footnote{$\Rightarrow \forall \, \, i \neq j \, \, \Sigma_{\bar{\bm \mu}}^{(i,j)}=0 \, , \Sigma_{\mathbf s}^{(i,j)}=0 $}, leading to a likelihood and a prior factorised respectively 
\begin{equation}
P(\mathbf x) = \prod_{j=1}^{D} \mathcal{N}(x_{j}; \bar{\mu}_{j}, \Sigma_{\bar{\bm \mu}}^{(j,j)})
\end{equation}
\begin{equation}
P(\mathbf s | \mathbf x) = \prod_{i=1}^{S} \mathcal{N}(s_{i}; g_{i}(\mathbf x), \Sigma_{\mathbf s}^{(i,i)})
\end{equation}
and a VFE with form
\begin{equation}
\begin{split}
F &\approx \sum_{i=1}^S \left[ \frac{(\varepsilon_{\mathbf s}^{(i)})^2}{2 \Sigma_{\mathbf s}^{(i,i)}} + \frac{1}{2} \ln \Sigma_{\mathbf s}^{(i,i)}  \right] + \sum_{j=1}^D \left[ \frac{(\varepsilon_{\bm \mu}^{(j)})^2}{2 \Sigma_{\bar{\bm \mu}}^{(j,j)}} + \frac{1}{2} \ln \Sigma_{\bar{\bm \mu}}^{(j,j)}  \right] \\
	&= \sum_{i=1}^S \left[ \frac{(\varepsilon_{\mathbf s}^{(i)})^2}{2 \Sigma_{\mathbf s}^{(i,i)}} \right] + \sum_{j=1}^D \left[ \frac{(\varepsilon_{\bm \mu}^{(j)})^2}{2 \Sigma_{\bm \mu}^{(j,j)}} \right] + \frac{1}{2} \ln \left( \det \bm{\hat{\Sigma}}_{\bar{\bm \mu}} \det \bm{\hat{\Sigma}}_{s} \right) \, .
\end{split}
\end{equation}


\newpage
\begin{equation}
\frac{d^2 x(t)}{dt^2} = - \omega^2 x(t)
\end{equation}


\begin{equation}
A^2=x^2(0)+\frac{v^2(0)}{\omega^2} \quad , \quad \tan (\varphi) = - \frac{v(0)}{\omega x(0)}
\end{equation}


\begin{equation}
x(t) = A \cos(\frac{t}{\phi} + \varphi)
\end{equation}

\begin{equation}
\frac{ d x_{new} }{ dt } = -x_{new} + \alpha x
\end{equation}

\begin{equation}
\mathbf x = \left[\begin{array}{c} x_0 \\ x_1 \\ x_2 \end{array}\right] 
	\quad , \quad \mathbf{\dot{x}} = \left[\begin{array}{c}
    \phi^2 x_1 \\
    -x_0 \\
    \alpha x_0 - x_2
    \end{array}\right] 
\end{equation}

\begin{equation}
\bm \mu = \left[\begin{array}{c} \mu_0 \\ \mu_1 \\ \mu_2 \end{array}\right] 
	\quad , \quad \bm{\dot{\mu}} = \left[\begin{array}{c}
    \phi^2 \mu_1 \\
    -\mu_0 \\
    \nu \mu_0 - \mu_2
    \end{array}\right] + \mathcal{N}(\bm \mu | 0, \Sigma_x) 
\end{equation}

\begin{equation}
\nu \, += -\frac{dF}{d\nu}
\end{equation}

\begin{equation}
\bm{\dot{\mu}} = \left[\begin{array}{c}
    \phi^2 \mu_1 \\
    -\mu_0 \\
    \nu \mu_0 - \mu_2
    \end{array}\right] + \mathcal{N}(\bm \mu | 0, \Sigma_x)  \\
\end{equation}
\begin{equation}
s_0 = \mu_2 + \mathcal{N}(s_0 | 0, \Sigma_{s_0}) 
\end{equation}

\begin{equation}
s_1 = f(\mu_0, \mu_1) + \mathcal{N}(s_1 | 0, \Sigma_{s_1}) 
\end{equation}


\begin{equation}
F \approx \frac{1}{2} \left[ \frac{(\dot{\mu_0}-\phi^2\mu_1)^2}{\Sigma_x} + \frac{(\dot{\mu_1}+\mu_0)^2}{\Sigma_x} + \frac{(\dot{\mu_2}+\mu_2-\alpha\mu_0)^2}{\Sigma_x} + \frac{(s_0-\mu_2)^2}{\Sigma_{s_0}} + \frac{(s_1-f(\mu_0, \mu_1))^2}{\Sigma_{s_1}}\right]
\end{equation}

\begin{equation}
\frac{dF}{d \left[\begin{matrix}\mu_{0}\\\mu_{1}\\\mu_{2}\end{matrix}\right]} = \left[\begin{matrix}
	\frac{\dot{\mu_1}+\mu_0}{\Sigma_x} - \alpha\frac{\dot{\mu_2}+\mu_2-\alpha\mu_0}{\Sigma_x} \\
	-\phi^2\frac{\dot{\mu_0}-\phi^2\mu_1}{\Sigma_x}\\
	\frac{\dot{\mu_2}+\mu_2-\alpha\mu_0}{\Sigma_x} - \frac{s-\mu_2}{\Sigma_s}\end{matrix}\right]
\end{equation}

\begin{equation}
\frac{dF}{d \left[\begin{matrix}\dot{\mu_0}\\\dot{\mu_1}\\\dot{\mu_2}\end{matrix}\right]}  = \left[\begin{matrix}
	\frac{\dot{\mu_0}-\phi^2\mu_1}{\Sigma_x} \\
	\frac{\dot{\mu_1}+\mu_0}{\Sigma_x}\\
	\frac{\dot{\mu_2}+\mu_2-\alpha\mu_0}{\Sigma_x}\end{matrix}\right]
\end{equation}

gradienti francesco
\begin{equation}
\begin{split}
F \approx &\frac{\frac{\nu^{2} \mu_{x_0}^{2}}{2} - \nu \mu_{x_0} \mu_{x_2} - \nu \mu_{x_0} d\mu_{x_2} + \frac{\mu_{x_0}^{2}}{2} + \mu_{x_0} d\mu_{x_1} + \frac{\mu_{x_1}^{2} \left(\phi^{2}\right)^{2}}{2} - \mu_{x_1} \phi^{2} d\mu_{x_0} + \frac{\mu_{x_2}^{2}}{2} + \mu_{x_2} d\mu_{x_2}}{\sigma_{x}} + \\
& \frac{\frac{d\mu_{x_0}^{2}}{2} + \frac{d\mu_{x_1}^{2}}{2} + \frac{d\mu_{x_2}^{2}}{2}}{\sigma_{x}} + \frac{\frac{\mu_{x_2}^{2}}{2} - \mu_{x_2} s + \frac{s^{2}}{2}}{\sigma_{s}^{2}}
\end{split}
\end{equation}

\begin{equation}
- \frac{d}{d \left[\begin{matrix}\mu_{x_0}\\\mu_{x_1}\\\mu_{x_2}\end{matrix}\right]} F = \left[\begin{matrix}- \frac{\nu^{2} \mu_{x_0} - \nu \mu_{x_2} - \nu d\mu_{x_2} + \mu_{x_0} + d\mu_{x_1}}{\sigma_{x}}\\- \frac{\mu_{x_1} \left(\phi^{2}\right)^{2} - \phi^{2} d\mu_{x_0}}{\sigma_{x}}\\- \frac{- \nu \mu_{x_0} + \mu_{x_2} + d\mu_{x_2}}{\sigma_{x}} - \frac{\mu_{x_2} - s}{\sigma_{s}^{2}}\end{matrix}\right]
\end{equation}

\begin{equation}
- \frac{d}{d \left[\begin{matrix}d\mu_{x_0}\\d\mu_{x_1}\\d\mu_{x_2}\end{matrix}\right]} F = \left[\begin{matrix}- \frac{- \mu_{x_1} \phi^{2} + d\mu_{x_0}}{\sigma_{x}}\\- \frac{\mu_{x_0} + d\mu_{x_1}}{\sigma_{x}}\\- \frac{- \nu \mu_{x_0} + \mu_{x_2} + d\mu_{x_2}}{\sigma_{x}}\end{matrix}\right]
\end{equation}


\begin{equation}
\dot{\mu} = f(\mu) + \mathcal{N}(\dot{\mu}|0,\Sigma_{\dot{\mu}})
\end{equation}

\begin{equation}
\ddot{\mu} = \frac{\partial f}{\partial \mu} \dot{\mu} + \mathcal{N}(\ddot{\mu}|0,\Sigma_{\ddot{\mu}})
\end{equation}

\begin{equation}
\begin{split}
\mu(t+dt) &= \mu(t) + dt \, (\dot{\mu}(t) + \eta \frac{dF}{d\mu} ) \\
\dot{\mu}(t+dt) &= \dot{\mu}(t) + dt \, 	(\ddot{\mu}(t) + \eta \frac{dF}{d \dot{\mu}}) \\
\ddot{\mu}(t+dt) &= \ddot{\mu}(t) + dt \, (\eta \frac{dF}{d \ddot{\mu}}) \\
\end{split}
\end{equation}

\begin{equation}
\bm \mu = \left[ \begin{matrix} \mu_0 \\ \mu_1 \end{matrix} \right]
\end{equation}

\begin{equation}
\dot{\bm \mu} = \left[ \begin{matrix} \dot{\mu}_0 \\ \dot{\mu}_1 \end{matrix} \right] =  f\left( \left[ \begin{matrix} \mu_0 \\ \mu_1 \end{matrix} \right] \right) + \mathcal{N}(\dot{\bm \mu}|0,\Sigma_{\dot{\bm \mu}})
\end{equation}

\begin{equation}
\bm \mu(t+dt) = \left[ \begin{matrix} \mu_0(t+dt) \\ \mu_1(t+dt) \end{matrix} \right] = 
	\left[ \begin{matrix} \mu_0(t) + dt \, (\dot{\mu}_0 - \eta \frac{dF}{d\mu_0}) \\
						  \mu_1(t) + dt \, (\dot{\mu}_1 - \eta \frac{dF}{d\mu_1})
	\end{matrix} \right]
\end{equation}


\begin{equation}
\dot{\bm \mu}(t+dt) = \left[ \begin{matrix} \dot{\mu}_0(t+dt) \\ \dot{\mu}_1(t+dt) \end{matrix} \right] = 
	\left[ \begin{matrix} \dot{\mu}_0(t) + dt \, (- \eta \frac{dF}{d\mu_0}) \\
						  \dot{\mu}_1(t) + dt \, (- \eta \frac{dF}{d\mu_1})
	\end{matrix} \right]
\end{equation}

\begin{equation}
\left[ \begin{matrix} \mu_0(t+dt) \\ \mu_1(t+dt) \end{matrix} \right] = 
	\left[ \begin{matrix} \mu_0(t) + dt \, (\dot{\mu}_0 - \eta \frac{dF}{d\mu_0}) \\
						  \mu_1(t) + dt \, (\dot{\mu}_1 - \eta \frac{dF}{d\mu_1})
	\end{matrix} \right]
\end{equation}

\begin{equation}
\dot{\bm \mu} = \left[ \begin{matrix} \dot{\mu}_0 \\ \dot{\mu}_1 \end{matrix} \right] = \left[ \begin{matrix} 0 & 1 \\ -\omega^2 & 0 \end{matrix} \right]  \left[ \begin{matrix} \mu_0 \\ \mu_1 \end{matrix} \right] + \mathcal{N}(\dot{\bm \mu}|0,\Sigma_{\dot{\bm \mu}})
\end{equation}

\begin{equation}
\left[ \begin{matrix} \mu_0(t+dt) \\ \mu_1(t+dt)=\dot{\mu}_0(t+dt) \\ \mu_2(t+dt)=\dot{\mu}_1(t+dt) \end{matrix} \right] = 
	\left[ \begin{matrix} \mu_0(t) + dt \, (\mu_1(t) - \eta \frac{dF}{d\mu_0}) \\
						  \mu_1(t) + dt \, (\mu_2(t) - \eta \frac{dF}{d\mu_1}) \\
						  \mu_2(t) + dt \, ( - \eta	\frac{dF}{d\mu_2})
	\end{matrix} \right]
\end{equation}

\begin{equation}
\frac{1}{2 \pi \phi} \simeq 0.226
\end{equation}


\begin{equation}
T = 2 \pi \phi \simeq 4.442 \sim 5
\end{equation}


\begin{equation}
s_t = f(x,v) + \mathcal{N}(s_t|0, \Sigma_{s_t}) = g(v) h(x) + \mathcal{N}(s_t|0, \Sigma_{s_t}) 
\end{equation}




\newpage

\printbibliography



\end{document}


